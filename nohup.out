Multi-turn mode enabled, using multi-turn training function
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.39it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.39it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.06it/s]
wandb: Currently logged in as: jacobcd52 (training-saes) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /root/rg_obfuscation/wandb/run-20250718_064221-5368v2bb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run off_by_one-16
wandb: â­ï¸ View project at https://wandb.ai/training-saes/multiturn_island
wandb: ðŸš€ View run at https://wandb.ai/training-saes/multiturn_island/runs/5368v2bb
Loaded custom prompt for task 'largest_island' from registry
Traceback (most recent call last):
  File "/root/rg_obfuscation/train.py", line 12, in <module>
    train(args.config)
  File "/root/rg_obfuscation/reinforce/trainer.py", line 827, in train
    return train_multi_turn(config_path)
  File "/root/rg_obfuscation/reinforce/trainer.py", line 654, in train_multi_turn
    episode_results = run_batched_multi_turn_episodes(
  File "/root/rg_obfuscation/reinforce/trainer.py", line 497, in run_batched_multi_turn_episodes
    outputs = model.generate(
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 2625, in generate
    result = self._sample(
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 3651, in _sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
KeyboardInterrupt
Traceback (most recent call last):
  File "/root/rg_obfuscation/train.py", line 12, in <module>
    train(args.config)
  File "/root/rg_obfuscation/reinforce/trainer.py", line 827, in train
    return train_multi_turn(config_path)
  File "/root/rg_obfuscation/reinforce/trainer.py", line 654, in train_multi_turn
    episode_results = run_batched_multi_turn_episodes(
  File "/root/rg_obfuscation/reinforce/trainer.py", line 497, in run_batched_multi_turn_episodes
    outputs = model.generate(
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 2625, in generate
    result = self._sample(
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 3651, in _sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
KeyboardInterrupt
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33moff_by_one-16[0m at: [34mhttps://wandb.ai/training-saes/multiturn_island/runs/5368v2bb[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250718_064221-5368v2bb/logs[0m
Multi-turn mode enabled, using multi-turn training function
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.51it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.50it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.22it/s]
wandb: Currently logged in as: jacobcd52 (training-saes) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /root/rg_obfuscation/wandb/run-20250718_090551-qjokjgby
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run off_by_one-17
wandb: â­ï¸ View project at https://wandb.ai/training-saes/multiturn_island
wandb: ðŸš€ View run at https://wandb.ai/training-saes/multiturn_island/runs/qjokjgby
Loaded custom prompt for task 'largest_island' from registry
Traceback (most recent call last):
  File "/root/rg_obfuscation/train.py", line 12, in <module>
    train(args.config)
  File "/root/rg_obfuscation/reinforce/trainer.py", line 827, in train
    return train_multi_turn(config_path)
  File "/root/rg_obfuscation/reinforce/trainer.py", line 654, in train_multi_turn
    episode_results = run_batched_multi_turn_episodes(
  File "/root/rg_obfuscation/reinforce/trainer.py", line 497, in run_batched_multi_turn_episodes
    outputs = model.generate(
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 2625, in generate
    result = self._sample(
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 3609, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/models/qwen3/modeling_qwen3.py", line 570, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/models/qwen3/modeling_qwen3.py", line 454, in forward
    for decoder_layer in self.layers[: self.config.num_hidden_layers]:
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 291, in __getitem__
    return self.__class__(list(self._modules.values())[idx])
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 277, in __init__
    self += modules
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 318, in __iadd__
    return self.extend(modules)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 400, in extend
    self.add_module(str(offset + i), module)
KeyboardInterrupt
Traceback (most recent call last):
  File "/root/rg_obfuscation/train.py", line 12, in <module>
    train(args.config)
  File "/root/rg_obfuscation/reinforce/trainer.py", line 827, in train
    return train_multi_turn(config_path)
  File "/root/rg_obfuscation/reinforce/trainer.py", line 654, in train_multi_turn
    episode_results = run_batched_multi_turn_episodes(
  File "/root/rg_obfuscation/reinforce/trainer.py", line 497, in run_batched_multi_turn_episodes
    outputs = model.generate(
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 2625, in generate
    result = self._sample(
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 3609, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/models/qwen3/modeling_qwen3.py", line 570, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/models/qwen3/modeling_qwen3.py", line 454, in forward
    for decoder_layer in self.layers[: self.config.num_hidden_layers]:
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 291, in __getitem__
    return self.__class__(list(self._modules.values())[idx])
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 277, in __init__
    self += modules
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 318, in __iadd__
    return self.extend(modules)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 400, in extend
    self.add_module(str(offset + i), module)
KeyboardInterrupt
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33moff_by_one-17[0m at: [34mhttps://wandb.ai/training-saes/multiturn_island/runs/qjokjgby[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250718_090551-qjokjgby/logs[0m
Multi-turn mode enabled, using multi-turn training function
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.32it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.34it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.99it/s]
wandb: Currently logged in as: jacobcd52 (training-saes) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /root/rg_obfuscation/wandb/run-20250718_093113-04o8u0dd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run off_by_one-20
wandb: â­ï¸ View project at https://wandb.ai/training-saes/multiturn_island
wandb: ðŸš€ View run at https://wandb.ai/training-saes/multiturn_island/runs/04o8u0dd
Multi-turn mode enabled, using multi-turn training function
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.48it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.38it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.06it/s]
wandb: Currently logged in as: jacobcd52 (training-saes) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /root/rg_obfuscation/wandb/run-20250718_170921-z1i9dvau
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run off_by_one-21
wandb: â­ï¸ View project at https://wandb.ai/training-saes/multiturn_island
wandb: ðŸš€ View run at https://wandb.ai/training-saes/multiturn_island/runs/z1i9dvau
Loaded custom prompt for task 'largest_island' from registry
Error during training: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33moff_by_one-21[0m at: [34mhttps://wandb.ai/training-saes/multiturn_island/runs/z1i9dvau[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250718_170921-z1i9dvau/logs[0m
Multi-turn mode enabled, using multi-turn training function
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.33it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.36it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.01it/s]
wandb: Currently logged in as: jacobcd52 (training-saes) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /root/rg_obfuscation/wandb/run-20250718_171030-wvvrysr5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run off_by_one-23
wandb: â­ï¸ View project at https://wandb.ai/training-saes/multiturn_island
wandb: ðŸš€ View run at https://wandb.ai/training-saes/multiturn_island/runs/wvvrysr5
Loaded custom prompt for task 'largest_island' from registry
Traceback (most recent call last):
  File "/root/rg_obfuscation/train.py", line 12, in <module>
    train(args.config)
  File "/root/rg_obfuscation/reinforce/trainer.py", line 827, in train
    return train_multi_turn(config_path)
  File "/root/rg_obfuscation/reinforce/trainer.py", line 654, in train_multi_turn
    episode_results = run_batched_multi_turn_episodes(
  File "/root/rg_obfuscation/reinforce/trainer.py", line 497, in run_batched_multi_turn_episodes
    outputs = model.generate(
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 2625, in generate
    result = self._sample(
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 3651, in _sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
KeyboardInterrupt
Traceback (most recent call last):
  File "/root/rg_obfuscation/train.py", line 12, in <module>
    train(args.config)
  File "/root/rg_obfuscation/reinforce/trainer.py", line 827, in train
    return train_multi_turn(config_path)
  File "/root/rg_obfuscation/reinforce/trainer.py", line 654, in train_multi_turn
    episode_results = run_batched_multi_turn_episodes(
  File "/root/rg_obfuscation/reinforce/trainer.py", line 497, in run_batched_multi_turn_episodes
    outputs = model.generate(
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 2625, in generate
    result = self._sample(
  File "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py", line 3651, in _sample
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
KeyboardInterrupt
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33moff_by_one-23[0m at: [34mhttps://wandb.ai/training-saes/multiturn_island/runs/wvvrysr5[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250718_171030-wvvrysr5/logs[0m
